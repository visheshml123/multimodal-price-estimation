{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9f5021-0127-4c73-80d4-1eb47cb34a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from copy import deepcopy\n",
    "import joblib # Used for saving tree-based models and transformers\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration (Must match your original script) ---\n",
    "TRAIN_CSV_PATH = '/Users/visheshbishnoi/Desktop/amazon/code/train_with_engineered_features.csv'\n",
    "TRAIN_TEXT_EMBEDDINGS_PATH = '/Users/visheshbishnoi/Desktop/amazon/code/train_text_embeddings_CUSTOM.npy'\n",
    "TRAIN_IMG_EMBEDDINGS_PATH = '/Users/visheshbishnoi/Desktop/amazon/code/train_image_embeddings_siglip_with_id .npy'\n",
    "TEXT_PCA_COMPONENTS = 128\n",
    "IMG_PCA_COMPONENTS = 128\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --- Model Hyperparameters (Must match your original script) ---\n",
    "# Note: Removed early_stopping related params as we train on full data now\n",
    "lgbm_params = {'objective': 'regression_l1', 'metric': 'mae', 'n_estimators': 2500, 'learning_rate': 0.01, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'lambda_l1': 0.1, 'lambda_l2': 0.1, 'num_leaves': 40, 'verbose': -1, 'n_jobs': -1, 'seed': 42, 'boosting_type': 'gbdt', 'device': 'cpu'}\n",
    "xgb_params = {'objective': 'reg:squarederror', 'eval_metric': 'mae', 'n_estimators': 2500, 'learning_rate': 0.01, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.1, 'lambda': 0.1, 'alpha': 0.1, 'max_depth': 7, 'n_jobs': -1, 'seed': 42, 'tree_method': 'hist', 'device': 'cpu' if DEVICE.type == 'cpu' else 'cuda'}\n",
    "cat_params = {'iterations': 2500, 'learning_rate': 0.01, 'depth': 8, 'loss_function': 'MAE', 'eval_metric': 'MAE', 'random_seed': 42, 'verbose': 0, 'allow_writing_files': False, 'task_type': 'CPU' if DEVICE.type == 'cpu' else 'GPU'}\n",
    "mlp_params = {'batch_size': 256, 'epochs': 15, 'learning_rate': 1e-3, 'weight_decay': 0.01} # Reduced epochs for full training\n",
    "# Note: For full training, MLP training complexity is reduced or patience removed.\n",
    "\n",
    "# --- PyTorch MLP Model Definition (Reused) ---\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y): self.X, self.y = torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1))\n",
    "    def forward(self, x): return self.model(x).squeeze(-1)\n",
    "\n",
    "# --- 1. Load and Preprocess ALL Data (Reused logic) ---\n",
    "print(\"Loading and preparing ALL data for final training...\")\n",
    "train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "aligned_text_embed = np.load(TRAIN_TEXT_EMBEDDINGS_PATH)\n",
    "train_img_embed_data = np.load(TRAIN_IMG_EMBEDDINGS_PATH, allow_pickle=True)\n",
    "id_to_img_embedding_map = {item['sample_id']: item['embedding'] for item in train_img_embed_data if isinstance(item, dict) and 'sample_id' in item and 'embedding' in item}\n",
    "img_embedding_dim = next(iter(id_to_img_embedding_map.values())).shape[0]\n",
    "aligned_img_embed = np.array([id_to_img_embedding_map.get(sid, np.zeros(img_embedding_dim)) for sid in train_df['sample_id']])\n",
    "\n",
    "if 'log_price' not in train_df.columns and 'price_log' in train_df.columns: train_df.rename(columns={'price_log': 'log_price'}, inplace=True)\n",
    "is_valid_mask = train_df['log_price'].notna()\n",
    "train_df = train_df[is_valid_mask].reset_index(drop=True)\n",
    "train_text_embed = aligned_text_embed[is_valid_mask]\n",
    "train_img_embed = aligned_img_embed[is_valid_mask]\n",
    "\n",
    "numerical_features =  ['value', 'ipq', 'value_per_item','is_organic', 'is_sugar_free','is_premium_keyword',\n",
    "    'is_dietary_specific']\n",
    "categorical_features = ['unit_standardized', 'brand_cleaned']\n",
    "for col in numerical_features: train_df[col] = train_df[col].fillna(train_df[col].median())\n",
    "train_numerical_df = train_df[numerical_features]\n",
    "train_categorical_df = train_df[categorical_features].fillna('unknown').astype(str)\n",
    "y = train_df['log_price'].values\n",
    "\n",
    "# Fit and Transform Preprocessing Objects on ALL data\n",
    "print(\"Fitting PCA and Label Encoders on ALL data...\")\n",
    "text_pca = PCA(n_components=TEXT_PCA_COMPONENTS, random_state=42)\n",
    "train_text_pca = text_pca.fit_transform(train_text_embed)\n",
    "img_pca = PCA(n_components=IMG_PCA_COMPONENTS, random_state=42)\n",
    "train_img_pca = img_pca.fit_transform(train_img_embed)\n",
    "\n",
    "# Fit Label Encoders\n",
    "train_categorical_le_list = []\n",
    "le_encoders = {}\n",
    "for i, col in enumerate(categorical_features):\n",
    "    le = LabelEncoder()\n",
    "    encoded_col = le.fit_transform(train_categorical_df[col]).reshape(-1, 1)\n",
    "    train_categorical_le_list.append(encoded_col)\n",
    "    le_encoders[col] = le\n",
    "train_categorical_le = np.hstack(train_categorical_le_list)\n",
    "\n",
    "# Combine ALL features into the final training matrix (X_train_full)\n",
    "X_train_full = pd.concat([pd.DataFrame(train_numerical_df.values),\n",
    "                          pd.DataFrame(train_categorical_le),\n",
    "                          pd.DataFrame(train_text_pca),\n",
    "                          pd.DataFrame(train_img_pca)], axis=1)\n",
    "X_train_full.columns = [str(i) for i in range(X_train_full.shape[1])]\n",
    "cat_feature_indices = list(range(train_numerical_df.shape[1], train_numerical_df.shape[1] + train_categorical_le.shape[1]))\n",
    "print(f\"Final training matrix shape: {X_train_full.shape}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- 2. Train and Save Final Base Models (Level 1) ---\n",
    "\n",
    "# --- A. LightGBM ---\n",
    "print(\"Training and Saving Final LightGBM Model...\")\n",
    "final_model_lgbm = lgb.LGBMRegressor(**lgbm_params)\n",
    "final_model_lgbm.fit(X_train_full, y)\n",
    "joblib.dump(final_model_lgbm, 'final_model_lgbm.joblib')\n",
    "print(\"LightGBM saved.\")\n",
    "\n",
    "# --- B. XGBoost ---\n",
    "print(\"Training and Saving Final XGBoost Model...\")\n",
    "final_model_xgb = xgb.XGBRegressor(**xgb_params)\n",
    "final_model_xgb.fit(X_train_full, y, verbose=False)\n",
    "joblib.dump(final_model_xgb, 'final_model_xgb.joblib')\n",
    "print(\"XGBoost saved.\")\n",
    "\n",
    "# --- C. CatBoost ---\n",
    "print(\"Training and Saving Final CatBoost Model...\")\n",
    "final_model_cat = cb.CatBoostRegressor(**cat_params)\n",
    "final_model_cat.fit(X_train_full, y, cat_features=cat_feature_indices, verbose=0)\n",
    "joblib.dump(final_model_cat, 'final_model_cat.joblib')\n",
    "print(\"CatBoost saved.\")\n",
    "\n",
    "# --- D. MLP (Neural Network) ---\n",
    "print(\"Training and Saving Final MLP Model and StandardScaler...\")\n",
    "# Fit StandardScaler on the *full* final feature matrix\n",
    "scaler_nn = StandardScaler()\n",
    "X_train_nn = scaler_nn.fit_transform(X_train_full.values)\n",
    "train_loader = DataLoader(TabularDataset(X_train_nn, y), batch_size=mlp_params['batch_size'], shuffle=True)\n",
    "\n",
    "final_model_mlp = MLPModel(X_train_nn.shape[1]).to(DEVICE)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(final_model_mlp.parameters(), lr=mlp_params['learning_rate'], weight_decay=mlp_params['weight_decay'])\n",
    "\n",
    "# Simple training loop for full data (no validation/early stopping)\n",
    "for epoch in range(mlp_params['epochs']):\n",
    "    final_model_mlp.train()\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = final_model_mlp(batch_X.to(DEVICE))\n",
    "        loss = criterion(outputs, batch_y.to(DEVICE))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"MLP Epoch {epoch+1}/{mlp_params['epochs']} finished.\")\n",
    "\n",
    "torch.save(final_model_mlp.state_dict(), 'final_model_mlp.pt')\n",
    "joblib.dump(scaler_nn, 'final_scaler_nn.joblib')\n",
    "print(\"MLP model state dict and StandardScaler saved.\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# --- 3. Save Preprocessing Objects ---\n",
    "print(\"Saving final PCA transformers and Label Encoders...\")\n",
    "joblib.dump(text_pca, 'final_text_pca.joblib')\n",
    "joblib.dump(img_pca, 'final_img_pca.joblib')\n",
    "joblib.dump(le_encoders, 'final_label_encoders.joblib')\n",
    "print(\"Preprocessing objects saved.\")\n",
    "print(\"\\nALL FINAL BASE MODELS AND TRANSFORMERS ARE READY FOR TEST PREDICTION.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
