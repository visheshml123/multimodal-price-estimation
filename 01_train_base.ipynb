{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c70fc7-ac25-45a6-8ae6-8b9bddaf0240",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import catboost as cb\n",
    "from copy import deepcopy\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration ---\n",
    "TRAIN_CSV_PATH = '/Users/visheshbishnoi/Desktop/amazon/code/train_with_engineered_features.csv'\n",
    "TRAIN_TEXT_EMBEDDINGS_PATH = '/Users/visheshbishnoi/Desktop/amazon/code/train_text_embeddings_CUSTOM.npy'\n",
    "TRAIN_IMG_EMBEDDINGS_PATH = '/Users/visheshbishnoi/Desktop/amazon/code/train_image_embeddings_siglip_with_id .npy'\n",
    "N_SPLITS = 5\n",
    "TEXT_PCA_COMPONENTS = 128\n",
    "IMG_PCA_COMPONENTS = 128\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --- Model Hyperparameters ---\n",
    "lgbm_params = {'objective': 'regression_l1', 'metric': 'mae', 'n_estimators': 2500, 'learning_rate': 0.01, 'feature_fraction': 0.8, 'bagging_fraction': 0.8, 'bagging_freq': 1, 'lambda_l1': 0.1, 'lambda_l2': 0.1, 'num_leaves': 40, 'verbose': -1, 'n_jobs': -1, 'seed': 42, 'boosting_type': 'gbdt', 'device': 'cpu'}\n",
    "xgb_params = {'objective': 'reg:squarederror', 'eval_metric': 'mae', 'n_estimators': 2500, 'learning_rate': 0.01, 'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0.1, 'lambda': 0.1, 'alpha': 0.1, 'max_depth': 7, 'n_jobs': -1, 'seed': 42, 'tree_method': 'hist', 'early_stopping_rounds': 100, 'device': 'cpu' if DEVICE.type == 'cpu' else 'cuda'}\n",
    "cat_params = {'iterations': 2500, 'learning_rate': 0.01, 'depth': 8, 'loss_function': 'MAE', 'eval_metric': 'MAE', 'random_seed': 42, 'verbose': 0, 'allow_writing_files': False, 'task_type': 'CPU' if DEVICE.type == 'cpu' else 'GPU'}\n",
    "mlp_params = {'batch_size': 256, 'epochs': 100, 'learning_rate': 1e-3, 'weight_decay': 0.01, 'patience': 5, 'scheduler_patience': 3}\n",
    "\n",
    "\n",
    "# --- SMAPE Metric ---\n",
    "def smape(y_true, y_pred):\n",
    "    numerator = np.abs(y_pred - y_true)\n",
    "    denominator = (np.abs(y_true) + np.abs(y_pred)) / 2\n",
    "    ratio = np.where(denominator == 0, 0, numerator / denominator)\n",
    "    return np.mean(ratio) * 100\n",
    "\n",
    "# --- Load and Prepare All Data ---\n",
    "print(\"Loading and preparing all data...\")\n",
    "train_df = pd.read_csv(TRAIN_CSV_PATH)\n",
    "\n",
    "# Simplified loading for the direct .npy array format\n",
    "print(\"Loading text embeddings from .npy file...\")\n",
    "aligned_text_embed = np.load(TRAIN_TEXT_EMBEDDINGS_PATH)\n",
    "\n",
    "if len(train_df) != len(aligned_text_embed):\n",
    "    raise ValueError(f\"Row count mismatch between CSV ({len(train_df)}) and text embedding .npy ({len(aligned_text_embed)}).\")\n",
    "\n",
    "# Load and align image embeddings\n",
    "print(\"Loading image embeddings from .npy file...\")\n",
    "train_img_embed_data = np.load(TRAIN_IMG_EMBEDDINGS_PATH, allow_pickle=True)\n",
    "id_to_img_embedding_map = {item['sample_id']: item['embedding'] for item in train_img_embed_data if isinstance(item, dict) and 'sample_id' in item and 'embedding' in item}\n",
    "if not id_to_img_embedding_map:\n",
    "    raise ValueError(f\"Could not find any valid image embeddings in '{TRAIN_IMG_EMBEDDINGS_PATH}'.\")\n",
    "img_embedding_dim = next(iter(id_to_img_embedding_map.values())).shape[0]\n",
    "aligned_img_embed = np.array([id_to_img_embedding_map.get(sid, np.zeros(img_embedding_dim)) for sid in train_df['sample_id']])\n",
    "\n",
    "# Data preparation\n",
    "if 'log_price' not in train_df.columns and 'price_log' in train_df.columns: train_df.rename(columns={'price_log': 'log_price'}, inplace=True)\n",
    "if 'log_price' not in train_df.columns: raise KeyError(\"Target column ('log_price' or 'price_log') not found.\")\n",
    "is_valid_mask = train_df['log_price'].notna()\n",
    "train_df = train_df[is_valid_mask].reset_index(drop=True)\n",
    "train_text_embed = aligned_text_embed[is_valid_mask]\n",
    "train_img_embed = aligned_img_embed[is_valid_mask]\n",
    "numerical_features = ['value', 'ipq', 'value_per_item','is_organic', 'is_sugar_free','is_premium_keyword',\n",
    "    'is_dietary_specific']\n",
    "categorical_features = ['unit_standardized', 'brand_cleaned']\n",
    "for col in numerical_features: train_df[col] = train_df[col].fillna(train_df[col].median())\n",
    "train_numerical_df = train_df[numerical_features]\n",
    "train_categorical_df = train_df[categorical_features].fillna('unknown').astype(str)\n",
    "y = train_df['log_price'].values\n",
    "y_true_price = np.expm1(y)\n",
    "\n",
    "# Label Encode before CV loop\n",
    "print(\"Label Encoding all categorical features before cross-validation...\")\n",
    "train_categorical_le_list = []\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    encoded_col = le.fit_transform(train_categorical_df[col]).reshape(-1, 1)\n",
    "    train_categorical_le_list.append(encoded_col)\n",
    "train_categorical_le = np.hstack(train_categorical_le_list)\n",
    "print(\"Data loading and preparation complete.\")\n",
    "\n",
    "\n",
    "# --- PyTorch MLP Model Definition ---\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y): self.X, self.y = torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx): return self.X[idx], self.y[idx]\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(MLPModel, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 512), nn.BatchNorm1d(512), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256), nn.BatchNorm1d(256), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1))\n",
    "    def forward(self, x): return self.model(x).squeeze(-1)\n",
    "\n",
    "\n",
    "# --- Cross-Validation and Ensembling Loop ---\n",
    "kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "oof_lgbm, oof_xgb, oof_cat, oof_mlp = [np.zeros(len(train_df)) for _ in range(4)]\n",
    "print(f\"\\nStarting {N_SPLITS}-Fold CV for 4 models...\")\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(train_df)):\n",
    "    print(f\"\\n--- Fold {fold+1}/{N_SPLITS} ---\")\n",
    "    \n",
    "    # Data Prep for this Fold\n",
    "    text_pca = PCA(n_components=TEXT_PCA_COMPONENTS, random_state=42)\n",
    "    train_text_pca, val_text_pca = text_pca.fit_transform(train_text_embed[train_idx]), text_pca.transform(train_text_embed[val_idx])\n",
    "    img_pca = PCA(n_components=IMG_PCA_COMPONENTS, random_state=42)\n",
    "    train_img_pca, val_img_pca = img_pca.fit_transform(train_img_embed[train_idx]), img_pca.transform(train_img_embed[val_idx])\n",
    "    train_cat_le_fold, val_cat_le_fold = train_categorical_le[train_idx], train_categorical_le[val_idx]\n",
    "    y_train, y_val = y[train_idx], y[val_idx]\n",
    "\n",
    "    # Use pd.concat to create a DataFrame, preserving dtypes for CatBoost\n",
    "    X_train_tree = pd.concat([pd.DataFrame(train_numerical_df.iloc[train_idx].values), pd.DataFrame(train_cat_le_fold), pd.DataFrame(train_text_pca), pd.DataFrame(train_img_pca)], axis=1)\n",
    "    X_val_tree = pd.concat([pd.DataFrame(train_numerical_df.iloc[val_idx].values), pd.DataFrame(val_cat_le_fold), pd.DataFrame(val_text_pca), pd.DataFrame(val_img_pca)], axis=1)\n",
    "    X_train_tree.columns = [str(i) for i in range(X_train_tree.shape[1])]\n",
    "    X_val_tree.columns = [str(i) for i in range(X_val_tree.shape[1])]\n",
    "\n",
    "    # Train LightGBM\n",
    "    print(\"Training LightGBM...\")\n",
    "    model_lgbm = lgb.LGBMRegressor(**lgbm_params)\n",
    "    model_lgbm.fit(X_train_tree, y_train, eval_set=[(X_val_tree, y_val)], callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "    oof_lgbm[val_idx] = model_lgbm.predict(X_val_tree)\n",
    "    \n",
    "    # Train XGBoost\n",
    "    print(\"Training XGBoost...\")\n",
    "    model_xgb = xgb.XGBRegressor(**xgb_params)\n",
    "    model_xgb.fit(X_train_tree, y_train, eval_set=[(X_val_tree, y_val)], verbose=False)\n",
    "    oof_xgb[val_idx] = model_xgb.predict(X_val_tree)\n",
    "    \n",
    "    # Train CatBoost\n",
    "    print(\"Training CatBoost...\")\n",
    "    cat_feature_indices = list(range(train_numerical_df.shape[1], train_numerical_df.shape[1] + train_cat_le_fold.shape[1]))\n",
    "    model_cat = cb.CatBoostRegressor(**cat_params)\n",
    "    model_cat.fit(X_train_tree, y_train, cat_features=cat_feature_indices, eval_set=[(X_val_tree, y_val)], early_stopping_rounds=100, verbose=0)\n",
    "    oof_cat[val_idx] = model_cat.predict(X_val_tree)\n",
    "    \n",
    "    # Train MLP\n",
    "    print(\"Training MLP...\")\n",
    "    scaler = StandardScaler()\n",
    "    X_train_nn, X_val_nn = scaler.fit_transform(X_train_tree.values), scaler.transform(X_val_tree.values)\n",
    "    train_loader, val_loader = DataLoader(TabularDataset(X_train_nn, y_train), batch_size=mlp_params['batch_size'], shuffle=True), DataLoader(TabularDataset(X_val_nn, y_val), batch_size=mlp_params['batch_size'], shuffle=False)\n",
    "    model_mlp = MLPModel(X_train_nn.shape[1]).to(DEVICE)\n",
    "    criterion, optimizer = nn.MSELoss(), torch.optim.AdamW(model_mlp.parameters(), lr=mlp_params['learning_rate'], weight_decay=mlp_params['weight_decay'])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=mlp_params['scheduler_patience'])\n",
    "    best_val_loss, patience_counter, best_model_state = float('inf'), 0, None\n",
    "    for epoch in range(mlp_params['epochs']):\n",
    "        # MLP training loop\n",
    "        model_mlp.train()\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model_mlp(batch_X.to(DEVICE))\n",
    "            loss = criterion(outputs, batch_y.to(DEVICE))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        model_mlp.eval()\n",
    "        current_val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in val_loader:\n",
    "                outputs = model_mlp(batch_X.to(DEVICE))\n",
    "                loss = criterion(outputs, batch_y.to(DEVICE))\n",
    "                current_val_loss += loss.item()\n",
    "        avg_val_loss = current_val_loss / len(val_loader)\n",
    "        scheduler.step(avg_val_loss)\n",
    "        if avg_val_loss < best_val_loss: best_val_loss, patience_counter, best_model_state = avg_val_loss, 0, deepcopy(model_mlp.state_dict())\n",
    "        else: patience_counter += 1\n",
    "        if patience_counter >= mlp_params['patience']: break\n",
    "    if best_model_state: model_mlp.load_state_dict(best_model_state)\n",
    "    model_mlp.eval()\n",
    "    val_preds = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, _ in val_loader: val_preds.extend(model_mlp(batch_X.to(DEVICE)).cpu().numpy())\n",
    "    oof_mlp[val_idx] = np.array(val_preds)\n",
    "    \n",
    "    print(f\"Fold {fold+1} complete.\")\n",
    "\n",
    "# --- Final Results ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Cross-Validation Complete. Calculating final scores...\")\n",
    "print(\"=\"*50)\n",
    "score_lgbm, score_xgb, score_cat, score_mlp = [smape(y_true_price, np.expm1(oof)) for oof in [oof_lgbm, oof_xgb, oof_cat, oof_mlp]]\n",
    "print(f\"--- Individual Model OOF Scores ---\")\n",
    "print(f\"LightGBM SMAPE: {score_lgbm:.4f}\")\n",
    "print(f\"XGBoost SMAPE:  {score_xgb:.4f}\")\n",
    "print(f\"CatBoost SMAPE: {score_cat:.4f}\")\n",
    "print(f\"MLP SMAPE:      {score_mlp:.4f}\")\n",
    "print(\"\\n--- Ensemble Score ---\")\n",
    "ensemble_preds = (oof_lgbm + oof_xgb + oof_cat + oof_mlp) / 4\n",
    "score_ensemble = smape(y_true_price, np.expm1(ensemble_preds))\n",
    "print(f\"Simple Averaging Ensemble SMAPE: {score_ensemble:.4f}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# --- NEW: Save OOF Predictions for Stacking ---\n",
    "print(\"\\n--- Saving OOF Predictions for Stacking ---\")\n",
    "\n",
    "np.save('oof_lgbm_preds.npy', oof_lgbm)\n",
    "np.save('oof_xgb_preds.npy', oof_xgb)\n",
    "np.save('oof_cat_preds.npy', oof_cat)\n",
    "np.save('oof_mlp_preds.npy', oof_mlp)\n",
    "\n",
    "print(\"OOF predictions saved successfully.\")\n",
    "print(\"Files created: oof_lgbm_preds.npy, oof_xgb_preds.npy, oof_cat_preds.npy, oof_mlp_preds.npy\")\n",
    "# --- END NEW SECTION ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
